<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Kai Zhang</title>

  <meta name="author" content="Kai Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:25%;max-width:25%">
                  <a href="kai.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="kai.jpg"
                      class="hoverZoomLink"></a>
                </td>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kai Zhang</name>
                  </p>

                  Email: kaiz [AT] adobe [DOT] com or kz298 [AT] cornell [DOT] edu
                  <p style="text-align:center">
<!--                     <a href="https://www.icloud.com/iclouddrive/047q9QusECCp682jviMKkh7DA#kai_resume">CV</a>&nbsp/&nbsp -->
                    <!-- <a href="https://scholar.google.com/citations?user=6B7FPMoAAAAJ&hl=en">Google Scholar</a> -->
                    <a href="https://scholar.google.com/citations?hl=en&user=eVv0MrsAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
<!--                     <a href="https://www.linkedin.com/in/kai-zhang-53910214a/">LinkedIn</a> &nbsp/&nbsp -->
                    <a href="https://github.com/Kai-46/">Github</a>
                  </p>
                  <p>I am a Research Scientist at <a href="https://research.adobe.com/">Adobe Research</a>, working on
                    3D reconstruction and generation, inverse graphics problems. I recieved my
                    PhD
                    from <a href="https://tech.cornell.edu/">Cornell University</a> in 2022, where I worked with <a
                      href="http://www.cs.cornell.edu/~snavely/">Prof. Noah Snavely</a>.
                    Before PhD, I got my bachelor at <a
                      href="https://www.tsinghua.edu.cn/publish/thu2018en/index.html">Tsinghua University</a> in 2017.
                  </p>
                  <p style="color:magenta"></p>
                  <em>My latest active research area is <strong>generative 3D reconstructor</strong> that can 1) reconstruct from sparse
                    posed/unposed images; 2) hallucinate the unseen regions; 3) be generalizable; 4) be robust to
                  imperfect inputs, including lighting varitions, motion blur etc; 5) work on both object-level and scene-level.</em>
                  </p>
<!--                   <p style="color:magenta">
                    <em>Adobe internship/University collaboration: feel free to email me, if you share similar interests
                      in automatic 3D content creation from images, videos and texts! </em>
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  <heading>Research [This is outdated. See the full list of papers on <a href="https://scholar.google.com/citations?hl=en&user=eVv0MrsAAAAJ">Google Scholar</a>]</heading>
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [11]&nbsp&nbsp<papertitle>PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape
                    Prediction</papertitle>
                  <br>
                  <a href="https://totoro97.github.io/">Peng Wang</a>,
                  <a href="https://research.adobe.com/person/hao-tan/">Hao Tan</a>,
                  <a href="https://sai-bi.github.io/">Sai Bi</a>,
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://luanfujun.com/">Fujun Luan</a>,
                  <a href="https://research.adobe.com/person/kalyan-sunkavalli/">Kalyan Sunkavalli</a>,
                  <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>,
                  <a href="https://zexiangxu.github.io/">Zexiang Xu</a>,
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>
                  <br>
                  <em>ICLR</em>, 2024 (Spotlight) &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2311.12024">arxiv</a> /
                  <a href="https://totoro97.github.io/pf-lrm/">project page</a>
                  <br>
                  Given unposed sparse images, we propose a pose-free LRM that jointly reconstruct the full 3D shape
                  (even though the input images might only partially cover an object) while estimating the relative
                  camera poses of input images.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [10]&nbsp&nbsp<papertitle>DMV3D:Denoising Multi-View Diffusion using 3D Large Reconstruction Model
                  </papertitle>
                  <br>
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://research.adobe.com/person/hao-tan/">Hao Tan</a>,
                  <a href="https://luanfujun.com/">Fujun Luan</a>,
                  <a href="https://sai-bi.github.io/">Sai Bi</a>,
                  <a href="https://totoro97.github.io/">Peng Wang</a>,
                  <a href="https://jiahao.ai/">Jiahao Li</a>,
                  <a href="https://vivianszf.github.io/">Zifan Shi</a>,
                  <a href="https://research.adobe.com/person/kalyan-sunkavalli/">Kalyan Sunkavalli</a>,
                  <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://zexiangxu.github.io/">Zexiang Xu<sup>*</sup></a>,
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong><sup>*</sup></a> (Equal
                  advisory)
                  <br>
                  <em>ICLR</em>, 2024 (Spotlight) &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2311.09217">arxiv</a> /
                  <a href="https://justimyhxu.github.io/projects/dmv3d/">project page</a>
                  <br>
                  To model the uncertainty in the single-image 3D reconstruction problem, We propose to denoise
                  multi-view images using a 3D large reconstruction model (LRM) trained on a large-scale multi-view
                  dataset.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [9]&nbsp&nbsp<papertitle>Instant3D: Fast Text-to-3D with Sparse-View Generation and Large
                    Reconstruction Model</papertitle>
                  <br>
                  <a href="https://jiahao.ai/">Jiahao Li</a>,
                  <a href="https://research.adobe.com/person/hao-tan/">Hao Tan</a>,
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://zexiangxu.github.io/">Zexiang Xu</a>,
                  <a href="https://luanfujun.com/">Fujun Luan</a>,
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://yiconghong.me/">Yicong Hong</a>,
                  <a href="https://research.adobe.com/person/kalyan-sunkavalli/">Kalyan Sunkavalli</a>,
                  <a href="https://home.ttic.edu/~gregory/">Greg Shakhnarovich</a>
                  <a href="https://sai-bi.github.io/">Sai Bi</a>,
                  <br>
                  <em>ICLR</em>, 2024&nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2311.06214">arxiv</a> /
                  <a href="https://jiahao.ai/instant3d/">project page</a>
                  <br>
                  To achieve fast text-to-3D, we finetune SD-XL to generate multi-view images of an object given a text
                  prompt, then reconstruct the 3D shape from the generated images using a multi-view version of the LRM
                  model.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [8]&nbsp&nbsp<papertitle>LRM: Large Reconstruction Model for Single Image to 3D</papertitle>
                  <br>
                  <a href="https://yiconghong.me/">Yicong Hong</a>,
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://research.adobe.com/person/jiuxiang-gu/">Jiuxiang Gu</a>,
                  <a href="https://sai-bi.github.io/">Sai Bi</a>,
                  <a href="https://research.adobe.com/person/yang-zhou/">Yang Zhou</a>,
                  <a href="https://research.adobe.com/person/difan-liu/">Difan Liu</a>,
                  <a href="https://research.adobe.com/person/feng-liu/">Feng Liu</a>,
                  <a href="https://research.adobe.com/person/kalyan-sunkavalli/">Kalyan Sunkavalli</a>,
                  <a href="https://research.adobe.com/person/trung-bui/">Trung Bui</a>,
                  <a href="https://research.adobe.com/person/hao-tan/">Hao Tan</a>,
                  <br>
                  <em>ICLR</em>, 2024 (Oral)&nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2311.04400">arxiv</a> /
                  <a href="https://yiconghong.me/LRM/">project page</a>
                  <br>
                  We show that data + transformer paradigm can also naturally work for 3D reconstruction with the help
                  of large-scale multi-view datasets.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [7]&nbsp&nbsp<papertitle>Ray Conditioning: Trading Photo-Consistency for Photo-realism in Multi-view
                    Image Generation</papertitle>
                  <br>
                  <a href="https://echen01.github.io/">Eric Ming Chen</a>,
                  <a href="https://sholalkere.github.io/">Sidhanth Holalkere</a>,
                  <a href="https://yanruyu126.github.io/">Ruyu Yan</a>,
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="http://abedavis.com/">Abe Davis</a>
                  <br>
                  <em>ICCV</em>, 2023&nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2304.13681">arxiv</a> /
                  <a href="https://ray-cond.github.io">project page</a> /
                  <a href="https://github.com/echen01/ray-conditioning">code</a>
                  <br>
                  We propose to add ray conditioning to StyleGAN2 generator to enable 3D-aware view synthesis with high
                  photo-realism (at the cost of reduced consistency). This particularly suits the task of editing
                  viewpoints of static images.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [6]&nbsp&nbsp<papertitle>ARF: Artistic Radiance Fields</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://home.ttic.edu/~nickkolkin/home.html">Nick Kolkin</a>,
                  <a href="https://sai-bi.github.io/">Sai Bi</a>,
                  <a href="https://www.cs.cornell.edu/~fujun/">Fujun Luan</a>,
                  <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>,
                  <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                  <br>
                  <em>ECCV</em>, 2022&nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2206.06360">arxiv</a> /
                  <a href="https://www.cs.cornell.edu/projects/arf/">project page</a> /
                  <a href="https://github.com/Kai-46/ARF-svox2">code</a>
                  <br>
                  We propose to stylize the appearance of NeRF using Nearest Neighbor Feature Matching (NNFM) style loss
                  to create high-quality artistic 3D contents.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [5]&nbsp&nbsp<papertitle>IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from
                    Photometric Images</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://www.cs.cornell.edu/~fujun/">Fujun Luan</a>,
                  <a href="https://www.cs.cornell.edu/~zl548/">Zhengqi Li</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                  <br>
                  <em>CVPR</em>, 2022 (Oral)&nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2204.02232">arxiv</a> /
                  <a href="https://kai-46.github.io/IRON-website/">project page</a> /
                  <a href="https://github.com/Kai-46/IRON">code</a>
                  <br>
                  We propose a neural inverse rendering pipeline called IRON that operates on photometric images and
                  outputs high-quality 3D content in the format of triangle meshes and material textures readily
                  deployable in existing graphics pipelines.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [4]&nbsp&nbsp<papertitle>PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material
                    Editing and Relighting</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a><sup>*</sup>,
                  <a href="https://www.cs.cornell.edu/~fujun/">Fujun Luan<sup>*</sup></a>,
                  <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
                  <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a> (<sup>*</sup>Equal contribution)
                  <br>
                  <em>CVPR</em>, 2021 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2104.00674">arxiv</a> /
                  <a href="https://kai-46.github.io/PhySG-website/">project page</a> /
                  <a href="https://github.com/Kai-46/PhySG/">code</a>
                  <br>
                  We propose an end-to-end differentiable rendering pipeline that jointly estimates geometry, material
                  and lighting from multi-view images from scratch. It enables not just novel view synthesis, but also
                  relighting and material editing.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [3]&nbsp&nbsp<papertitle>NeRF++: Analyzing and Improving Neural Radiance Fields</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://griegler.github.io/">Gernot Riegler</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
                  <a href="http://vladlen.info/">Vladlen Koltun</a>
                  <br>
                  <em>arXiv preprint</em>, 2020 &nbsp
                  <br>
                  <a href="http://arxiv.org/abs/2010.07492">arxiv</a> /
                  <a href="https://github.com/Kai-46/nerfplusplus">code</a>
                  <br>
                  We analyze the shape-radiance ambiguity in NeRF, and extend NeRF to work with 360 unbounded scenes. At
                  the core of the method is the Inverted Sphere Parametrization (ISP) contracting an unbounded space to
                  a bounded one.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [2]&nbsp&nbsp<papertitle>Depth Sensing Beyond LiDAR Range</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a href="https://jiaxinxie97.github.io/Jiaxin-Xie/">Jiaxin Xie</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
                  <a href="https://cqf.io/">Qifeng Chen</a>
                  <br>
                  <em>CVPR</em>, 2020 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2004.03048">arxiv</a> /
                  <a href="https://kai-46.github.io/DepthSensing/">project page</a> /
                  <a href="https://github.com/Kai-46/DepthSensingBeyondLiDARRange">code</a>
                  <br>
                  We propose a novel cost-effective camera-based solution to sense the depth of distant objects that are
                  not reachable by typical LiDARs. This can be particularly helpful for heavily-weighted autonomous
                  trucks.
                </td>
              </tr>

              <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
                <td
                  style="padding-top:0px;padding-bottom:10px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [1]&nbsp&nbsp<papertitle>Leveraging Vision Reconstruction Pipelines for Satellite Imagery</papertitle>
                  <br>
                  <a href="https://kai-46.github.io/website/"><strong>Kai Zhang</strong></a>,
                  <a>Jin Sun</a>,
                  <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                  <br>
                  <em>ICCV 3DRW</em>, 2019 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/1910.02989">arxiv</a> /
                  <a href="https://kai-46.github.io/VisSat/">project page</a> /
                  <a href="https://kai-46.github.io/VisSat/#code">code</a>
                  <br>
                  We approximate satellite-specific RPC cameras with perspective cameras, and adapt vision
                  reconstruction pipelines (SfM+MVS) such that they can also process satellite images with competitive
                  accuracy and increased scalability.
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Services</heading>
                  <p>
                    Internships: Adobe (2022 Summer), Intel (2021 Summer, 2020 Summer), HKUST (2019 Summer), ICL (2016
                    Summer) <br>
                    Paper reviewer: ICCV, ECCV, CVPR, TVGG, TPAMI, SIGGRAPH, SIGGRAPH ASIA, NeurIPS, ICLR<br>
                    Seminar co-organizer: Cornell Graphics and Vision Seminar, 2021 Fall<br>
                    Talk speaker: Graphics And Mixed Environment Seminar (GAMES), 2021<br>
                    Teaching assistant: Deep Learning, 2022 Spring; Applied Machine Learning, 2021 Fall, 2020 Fall;
                    Introduction to Computer Vision, 2020 Spring, 2019 Spring
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

<!--           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td
                  style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  <heading>Open-source</heading>
                  <p>
                    In my spare time, I’ve been actively contributing to building an all-in-one open-source satellite
                    stereo
                    toolbox called <a href="https://github.com/Kai-46/SatelliteSfM">SatelliteSfM</a>, in order to
                    facilitate latest advances in 3D computer vision to satellite domain.
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Kudos to <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing his website template.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
